{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6585fa5-42b7-41ad-a83c-1b96abecdb69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast, col, rand\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add0d612-95da-46a4-a73b-f5bb0d7c35ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n현재 브로드캐스트 조인 임계값: 10485760b\n"
     ]
    }
   ],
   "source": [
    "# 브로드캐스트 조인 힌트 임계값 확인 (기본값: 10MB)\n",
    "broadcast_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"\\n현재 브로드캐스트 조인 임계값: {broadcast_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476efe9b-8451-40c9-81e6-debf09410d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+\n|dept_id|    dept_name|  location|\n+-------+-------------+----------+\n|      1| Department-1|Location-1|\n|      2| Department-2|Location-2|\n|      3| Department-3|Location-3|\n|      4| Department-4|Location-4|\n|      5| Department-5|Location-0|\n|      6| Department-6|Location-1|\n|      7| Department-7|Location-2|\n|      8| Department-8|Location-3|\n|      9| Department-9|Location-4|\n|     10|Department-10|Location-0|\n|     11|Department-11|Location-1|\n|     12|Department-12|Location-2|\n|     13|Department-13|Location-3|\n|     14|Department-14|Location-4|\n|     15|Department-15|Location-0|\n|     16|Department-16|Location-1|\n|     17|Department-17|Location-2|\n|     18|Department-18|Location-3|\n|     19|Department-19|Location-4|\n|     20|Department-20|Location-0|\n+-------+-------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 작은 테이블 생성 (부서 테이블) - 브로드캐스트 대상\n",
    "departments = [(i, f\"Department-{i}\", f\"Location-{i%5}\") for i in range(1, 101)]\n",
    "dept_df = spark.createDataFrame(departments, [\"dept_id\", \"dept_name\", \"location\"])\n",
    "    \n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee17020-5f21-4755-99c8-05c39a5d380f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------+------+\n|emp_id|       name|dept_id|salary|\n+------+-----------+-------+------+\n|     1| Employee-1|      2| 50001|\n|     2| Employee-2|      3| 50002|\n|     3| Employee-3|      4| 50003|\n|     4| Employee-4|      5| 50004|\n|     5| Employee-5|      6| 50005|\n|     6| Employee-6|      7| 50006|\n|     7| Employee-7|      8| 50007|\n|     8| Employee-8|      9| 50008|\n|     9| Employee-9|     10| 50009|\n|    10|Employee-10|     11| 50010|\n|    11|Employee-11|     12| 50011|\n|    12|Employee-12|     13| 50012|\n|    13|Employee-13|     14| 50013|\n|    14|Employee-14|     15| 50014|\n|    15|Employee-15|     16| 50015|\n|    16|Employee-16|     17| 50016|\n|    17|Employee-17|     18| 50017|\n|    18|Employee-18|     19| 50018|\n|    19|Employee-19|     20| 50019|\n|    20|Employee-20|     21| 50020|\n+------+-----------+-------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 큰 테이블 생성 (직원 테이블)\n",
    "# 1천만 건의 직원 데이터 생성\n",
    "num_employees = 10_000_000\n",
    "employees = [(i, \n",
    "    f\"Employee-{i}\", \n",
    "    1 + (i % 100),  # dept_id (1-100 사이)\n",
    "    50000 + (i % 50000))  # salary\n",
    "    for i in range(1, num_employees + 1)]\n",
    "emp_df = spark.createDataFrame(employees, [\"emp_id\", \"name\", \"dept_id\", \"salary\"])\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1bde7a-f7ac-4f2e-97ce-600002d3dd5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부서 테이블 크기: 100 행\n직원 테이블 크기: 10000000 행\n==================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"부서 테이블 크기: {dept_df.count()} 행\")\n",
    "print(f\"직원 테이블 크기: {emp_df.count()} 행\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e3e2c7-de93-4aae-b20f-c941ebc85033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[64]: 10000000"
     ]
    }
   ],
   "source": [
    "# 캐시하여 비교를 공정하게 만듦\n",
    "dept_df.cache().count()\n",
    "emp_df.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eeafc10-a0e2-41b6-b86e-ad33333ddb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 일반 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b35cc7b-5639-4bb4-b8f6-7e5ac4a3b2fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def perform_regular_join():\n",
    "    start_time = time.time()\n",
    "\n",
    "    regular_join = emp_df.join(\n",
    "        dept_df,\n",
    "        emp_df.dept_id == dept_df.dept_id\n",
    "    )\n",
    "        \n",
    "    # 실행 트리거\n",
    "    regular_count = regular_join.count()\n",
    "    regular_time = time.time() - start_time\n",
    "        \n",
    "    print(\"\\n일반 조인 실행:\")\n",
    "    print(f\"처리 시간: {regular_time:.2f}초\")\n",
    "    print(f\"결과 행 수: {regular_count}\")\n",
    "    print(\"\\n실행 계획:\")\n",
    "    regular_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b665ef-f27b-4e34-a5bd-9f9d3bc03a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n일반 조인 실행:\n처리 시간: 3.43초\n결과 행 수: 10000000\n\n실행 계획:\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastHashJoin [dept_id#14328L], [dept_id#14307L], Inner, BuildRight, false, true\n   :- Filter isnotnull(dept_id#14328L)\n   :  +- InMemoryTableScan [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], [isnotnull(dept_id#14328L)], false\n   :        +- InMemoryRelation [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], StorageLevel(disk, memory, deserialized, 1 replicas)\n   :              +- *(1) Scan ExistingRDD[emp_id#14326L,name#14327,dept_id#14328L,salary#14329L]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=15455]\n      +- Filter isnotnull(dept_id#14307L)\n         +- InMemoryTableScan [dept_id#14307L, dept_name#14308, location#14309], [isnotnull(dept_id#14307L)], false\n               +- InMemoryRelation [dept_id#14307L, dept_name#14308, location#14309], StorageLevel(disk, memory, deserialized, 1 replicas)\n                     +- *(1) Scan ExistingRDD[dept_id#14307L,dept_name#14308,location#14309]\n\n\n"
     ]
    }
   ],
   "source": [
    "perform_regular_join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffd9b37-4845-47af-8a8d-412eb0c865e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 브로드캐스트 자동 최적화 비활성화\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87f27c7-a0e6-439c-9b67-3a054ef8082b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n일반 조인 실행:\n처리 시간: 6.71초\n결과 행 수: 10000000\n\n실행 계획:\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [dept_id#14328L], [dept_id#14307L], Inner\n   :- Sort [dept_id#14328L ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(dept_id#14328L, 200), ENSURE_REQUIREMENTS, [plan_id=15847]\n   :     +- Filter isnotnull(dept_id#14328L)\n   :        +- InMemoryTableScan [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], [isnotnull(dept_id#14328L)], false\n   :              +- InMemoryRelation [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], StorageLevel(disk, memory, deserialized, 1 replicas)\n   :                    +- *(1) Scan ExistingRDD[emp_id#14326L,name#14327,dept_id#14328L,salary#14329L]\n   +- Sort [dept_id#14307L ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(dept_id#14307L, 200), ENSURE_REQUIREMENTS, [plan_id=15848]\n         +- Filter isnotnull(dept_id#14307L)\n            +- InMemoryTableScan [dept_id#14307L, dept_name#14308, location#14309], [isnotnull(dept_id#14307L)], false\n                  +- InMemoryRelation [dept_id#14307L, dept_name#14308, location#14309], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- *(1) Scan ExistingRDD[dept_id#14307L,dept_name#14308,location#14309]\n\n\n"
     ]
    }
   ],
   "source": [
    "perform_regular_join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a0076d-0364-4ea2-b022-4b273974b39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 브로드캐스트 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b11090-9846-406f-a13f-75b89fdfc2bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n브로드캐스트 조인 실행:\n처리 시간: 2.72초\n결과 행 수: 10000000\n\n실행 계획:\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastHashJoin [dept_id#14328L], [dept_id#14307L], Inner, BuildRight, false, true\n   :- Filter isnotnull(dept_id#14328L)\n   :  +- InMemoryTableScan [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], [isnotnull(dept_id#14328L)], false\n   :        +- InMemoryRelation [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], StorageLevel(disk, memory, deserialized, 1 replicas)\n   :              +- *(1) Scan ExistingRDD[emp_id#14326L,name#14327,dept_id#14328L,salary#14329L]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=16165]\n      +- Filter isnotnull(dept_id#14307L)\n         +- InMemoryTableScan [dept_id#14307L, dept_name#14308, location#14309], [isnotnull(dept_id#14307L)], false\n               +- InMemoryRelation [dept_id#14307L, dept_name#14308, location#14309], StorageLevel(disk, memory, deserialized, 1 replicas)\n                     +- *(1) Scan ExistingRDD[dept_id#14307L,dept_name#14308,location#14309]\n\n\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "    \n",
    "broadcast_join = emp_df.join(\n",
    "    broadcast(dept_df),\n",
    "    emp_df.dept_id == dept_df.dept_id\n",
    ")\n",
    "    \n",
    "# 실행 트리거\n",
    "broadcast_count = broadcast_join.count()\n",
    "broadcast_time = time.time() - start_time\n",
    "    \n",
    "print(\"\\n브로드캐스트 조인 실행:\")\n",
    "print(f\"처리 시간: {broadcast_time:.2f}초\")\n",
    "print(f\"결과 행 수: {broadcast_count}\")\n",
    "print(\"\\n실행 계획:\")\n",
    "broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649d6dae-95b5-4085-829b-86920cc288f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n결과 데이터 샘플:\n+------+----------+------------+----------+------+\n|emp_id|      name|   dept_name|  location|salary|\n+------+----------+------------+----------+------+\n|     1|Employee-1|Department-2|Location-2| 50001|\n|     2|Employee-2|Department-3|Location-3| 50002|\n|     3|Employee-3|Department-4|Location-4| 50003|\n|     4|Employee-4|Department-5|Location-0| 50004|\n|     5|Employee-5|Department-6|Location-1| 50005|\n+------+----------+------------+----------+------+\nonly showing top 5 rows\n\n\n부서별 평균 급여:\n+--------------+-----------+\n|     dept_name|avg(salary)|\n+--------------+-----------+\n|  Department-1|    74950.0|\n| Department-10|    74959.0|\n|Department-100|    75049.0|\n| Department-11|    74960.0|\n| Department-12|    74961.0|\n+--------------+-----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 결과 데이터 샘플 확인\n",
    "print(\"\\n결과 데이터 샘플:\")\n",
    "broadcast_join.select(\"emp_id\", \"name\", \"dept_name\", \"location\", \"salary\").show(5)\n",
    "    \n",
    "# 집계 쿼리로 조인 결과 활용 예시\n",
    "print(\"\\n부서별 평균 급여:\")\n",
    "broadcast_join.groupBy(\"dept_name\").agg({\"salary\": \"avg\"}).orderBy(\"dept_name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f296076-0e70-4883-96bc-7c50e359fb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dept_df.createOrReplaceTempView(\"dept\")\n",
    "emp_df.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c98a3c-4866-4a9c-86fc-d94a1c4841c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------+\n|emp_id|        name|    dept_name|\n+------+------------+-------------+\n|    25| Employee-25|Department-26|\n|    28| Employee-28|Department-29|\n|   125|Employee-125|Department-26|\n|   128|Employee-128|Department-29|\n|   225|Employee-225|Department-26|\n|   228|Employee-228|Department-29|\n|   325|Employee-325|Department-26|\n|   328|Employee-328|Department-29|\n|   425|Employee-425|Department-26|\n|   428|Employee-428|Department-29|\n|   525|Employee-525|Department-26|\n|   528|Employee-528|Department-29|\n|   625|Employee-625|Department-26|\n|   628|Employee-628|Department-29|\n|   725|Employee-725|Department-26|\n|   728|Employee-728|Department-29|\n|   825|Employee-825|Department-26|\n|   828|Employee-828|Department-29|\n|   925|Employee-925|Department-26|\n|   928|Employee-928|Department-29|\n+------+------------+-------------+\nonly showing top 20 rows\n\n== Parsed Logical Plan ==\n'Project ['e.emp_id, 'e.name, 'd.dept_name]\n+- 'Join Inner, ('e.dept_id = 'd.dept_id)\n   :- 'SubqueryAlias e\n   :  +- 'UnresolvedRelation [emp], [], false\n   +- 'SubqueryAlias d\n      +- 'UnresolvedRelation [dept], [], false\n\n== Analyzed Logical Plan ==\nemp_id: bigint, name: string, dept_name: string\nProject [emp_id#14326L, name#14327, dept_name#14308]\n+- Join Inner, (dept_id#14328L = dept_id#14307L)\n   :- SubqueryAlias e\n   :  +- SubqueryAlias emp\n   :     +- View (`emp`, [emp_id#14326L,name#14327,dept_id#14328L,salary#14329L])\n   :        +- LogicalRDD [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], false\n   +- SubqueryAlias d\n      +- SubqueryAlias dept\n         +- View (`dept`, [dept_id#14307L,dept_name#14308,location#14309])\n            +- LogicalRDD [dept_id#14307L, dept_name#14308, location#14309], false\n\n== Optimized Logical Plan ==\nProject [emp_id#14326L, name#14327, dept_name#14308]\n+- Join Inner, (dept_id#14328L = dept_id#14307L)\n   :- Project [emp_id#14326L, name#14327, dept_id#14328L]\n   :  +- Filter isnotnull(dept_id#14328L)\n   :     +- InMemoryRelation [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], StorageLevel(disk, memory, deserialized, 1 replicas)\n   :           +- *(1) Scan ExistingRDD[emp_id#14326L,name#14327,dept_id#14328L,salary#14329L]\n   +- Project [dept_id#14307L, dept_name#14308]\n      +- Filter isnotnull(dept_id#14307L)\n         +- InMemoryRelation [dept_id#14307L, dept_name#14308, location#14309], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- *(1) Scan ExistingRDD[dept_id#14307L,dept_name#14308,location#14309]\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [emp_id#14326L, name#14327, dept_name#14308]\n   +- SortMergeJoin [dept_id#14328L], [dept_id#14307L], Inner\n      :- Sort [dept_id#14328L ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(dept_id#14328L, 200), ENSURE_REQUIREMENTS, [plan_id=17613]\n      :     +- Filter isnotnull(dept_id#14328L)\n      :        +- InMemoryTableScan [emp_id#14326L, name#14327, dept_id#14328L], [isnotnull(dept_id#14328L)], false\n      :              +- InMemoryRelation [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], StorageLevel(disk, memory, deserialized, 1 replicas)\n      :                    +- *(1) Scan ExistingRDD[emp_id#14326L,name#14327,dept_id#14328L,salary#14329L]\n      +- Sort [dept_id#14307L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(dept_id#14307L, 200), ENSURE_REQUIREMENTS, [plan_id=17614]\n            +- Filter isnotnull(dept_id#14307L)\n               +- InMemoryTableScan [dept_id#14307L, dept_name#14308], [isnotnull(dept_id#14307L)], false\n                     +- InMemoryRelation [dept_id#14307L, dept_name#14308, location#14309], StorageLevel(disk, memory, deserialized, 1 replicas)\n                           +- *(1) Scan ExistingRDD[dept_id#14307L,dept_name#14308,location#14309]\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT e.emp_id,\n",
    "       e.name,\n",
    "       d.dept_name\n",
    "FROM   emp e\n",
    "JOIN   dept d\n",
    "ON     e.dept_id = d.dept_id\n",
    "\"\"\")\n",
    "df.show()\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a4488c-f108-459d-a2b6-f56c6b1e2c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+\n|emp_id|       name|    dept_name|\n+------+-----------+-------------+\n|     1| Employee-1| Department-2|\n|     2| Employee-2| Department-3|\n|     3| Employee-3| Department-4|\n|     4| Employee-4| Department-5|\n|     5| Employee-5| Department-6|\n|     6| Employee-6| Department-7|\n|     7| Employee-7| Department-8|\n|     8| Employee-8| Department-9|\n|     9| Employee-9|Department-10|\n|    10|Employee-10|Department-11|\n|    11|Employee-11|Department-12|\n|    12|Employee-12|Department-13|\n|    13|Employee-13|Department-14|\n|    14|Employee-14|Department-15|\n|    15|Employee-15|Department-16|\n|    16|Employee-16|Department-17|\n|    17|Employee-17|Department-18|\n|    18|Employee-18|Department-19|\n|    19|Employee-19|Department-20|\n|    20|Employee-20|Department-21|\n+------+-----------+-------------+\nonly showing top 20 rows\n\n== Parsed Logical Plan ==\n'UnresolvedHint BROADCAST, ['dept]\n+- 'Project ['e.emp_id, 'e.name, 'd.dept_name]\n   +- 'Join Inner, ('e.dept_id = 'd.dept_id)\n      :- 'SubqueryAlias e\n      :  +- 'UnresolvedRelation [emp], [], false\n      +- 'SubqueryAlias d\n         +- 'UnresolvedRelation [dept], [], false\n\n== Analyzed Logical Plan ==\nemp_id: bigint, name: string, dept_name: string\nProject [emp_id#14326L, name#14327, dept_name#14308]\n+- Join Inner, (dept_id#14328L = dept_id#14307L)\n   :- SubqueryAlias e\n   :  +- SubqueryAlias emp\n   :     +- View (`emp`, [emp_id#14326L,name#14327,dept_id#14328L,salary#14329L])\n   :        +- LogicalRDD [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], false\n   +- SubqueryAlias d\n      +- SubqueryAlias dept\n         +- View (`dept`, [dept_id#14307L,dept_name#14308,location#14309])\n            +- LogicalRDD [dept_id#14307L, dept_name#14308, location#14309], false\n\n== Optimized Logical Plan ==\nProject [emp_id#14326L, name#14327, dept_name#14308]\n+- Join Inner, (dept_id#14328L = dept_id#14307L)\n   :- Project [emp_id#14326L, name#14327, dept_id#14328L]\n   :  +- Filter isnotnull(dept_id#14328L)\n   :     +- InMemoryRelation [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], StorageLevel(disk, memory, deserialized, 1 replicas)\n   :           +- *(1) Scan ExistingRDD[emp_id#14326L,name#14327,dept_id#14328L,salary#14329L]\n   +- Project [dept_id#14307L, dept_name#14308]\n      +- Filter isnotnull(dept_id#14307L)\n         +- InMemoryRelation [dept_id#14307L, dept_name#14308, location#14309], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- *(1) Scan ExistingRDD[dept_id#14307L,dept_name#14308,location#14309]\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [emp_id#14326L, name#14327, dept_name#14308]\n   +- BroadcastHashJoin [dept_id#14328L], [dept_id#14307L], Inner, BuildRight, false, true\n      :- Filter isnotnull(dept_id#14328L)\n      :  +- InMemoryTableScan [emp_id#14326L, name#14327, dept_id#14328L], [isnotnull(dept_id#14328L)], false\n      :        +- InMemoryRelation [emp_id#14326L, name#14327, dept_id#14328L, salary#14329L], StorageLevel(disk, memory, deserialized, 1 replicas)\n      :              +- *(1) Scan ExistingRDD[emp_id#14326L,name#14327,dept_id#14328L,salary#14329L]\n      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=17838]\n         +- Filter isnotnull(dept_id#14307L)\n            +- InMemoryTableScan [dept_id#14307L, dept_name#14308], [isnotnull(dept_id#14307L)], false\n                  +- InMemoryRelation [dept_id#14307L, dept_name#14308, location#14309], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- *(1) Scan ExistingRDD[dept_id#14307L,dept_name#14308,location#14309]\n\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760b\")\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(dept) */\n",
    "       e.emp_id,\n",
    "       e.name,\n",
    "       d.dept_name\n",
    "    FROM   emp e\n",
    "    JOIN   dept d\n",
    "    ON     e.dept_id = d.dept_id\n",
    "\"\"\")\n",
    "df.show()\n",
    "\n",
    "df.explain(True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Shuffling JOIN과 Broadcast JOIN 학습하기",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
